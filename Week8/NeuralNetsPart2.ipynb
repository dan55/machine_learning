{
 "metadata": {
  "name": "",
  "signature": "sha256:2c3ce4b9c1a1fa8dd14260b773ba48d2ecf11f555fbc9d5d6e114d41b63bd340"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Digit Classification with Neural Networks "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interest in neural nets, and in particular those with more than than one hidden layer, has been surging in recent years.  In this notebook we will be revisiting the problem of digit classification on the MNIST data.  We will be introducing a new Python library, Theano, for working with neural nets.  Theano is a popular choice as the same code can be run on either CPUs or GPUs.  GPUs greatly speed up the training and prediction, and easily available (Amazon even offers GPU machines on EC2)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Part 1: Basics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In part 1, we will review the basics of neural nets, and introduce Theano.  In part 2, we will investigate more advanced topics in neural nets, including  deep learning.  I'd encourage you to read this paper as well as a supplementary explanation of Theano (http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "# Familiar libraries.\n",
      "import numpy as np\n",
      "from sklearn.datasets import fetch_mldata\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import time\n",
      "\n",
      "# Take a moment to install Theano.  We will use it for building neural networks.\n",
      "import theano \n",
      "from theano import tensor as T\n",
      "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
      "print theano.config.device # We're using CPUs (for now)\n",
      "print theano.config.floatX # Should be 64 bit for CPUs\n",
      "\n",
      "np.random.seed(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cpu\n",
        "float64\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Repeating steps from Project 1 to prepare mnist dataset. \n",
      "mnist = fetch_mldata('MNIST original', data_home='~/datasets/mnist')\n",
      "X, Y = mnist.data, mnist.target\n",
      "X = X / 255.0\n",
      "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
      "X, Y = X[shuffle], Y[shuffle]\n",
      "test_data, test_labels = X[60000:], Y[60000:]\n",
      "train_data, train_labels = X[:60000], Y[:60000]\n",
      "numFeatures = train_data[1].size\n",
      "numTrainExamples = train_data.shape[0]\n",
      "numTestExamples = test_data.shape[0]\n",
      "print 'Features = %d' %(numFeatures)\n",
      "print 'Train set = %d' %(numTrainExamples)\n",
      "print 'Test set = %d' %(numTestExamples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Features = 784\n",
        "Train set = 60000\n",
        "Test set = 10000\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert labels into a set of binary variables, one for each class (sometimes called a 1-of-n encoding).  \n",
      "# This makes working with NNs easier: there will be one output node for each class.\n",
      "def binarizeY(data):\n",
      "    binarized_data = np.zeros((data.size,10))\n",
      "    for j in range(0,data.size):\n",
      "        feature = data[j:j+1]\n",
      "        i = feature.astype(np.int64) \n",
      "        binarized_data[j,i]=1\n",
      "    return binarized_data\n",
      "train_labels_b = binarizeY(train_labels)\n",
      "test_labels_b = binarizeY(test_labels)\n",
      "numClasses = train_labels_b[1].size\n",
      "print 'Classes = %d' %(numClasses)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Classes = 10\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Lets start with a simple KNN model to establish a baseline accuracy.\n",
      "# Question: You've seen a number of different machine learning algos.  What's your intuition about KNN scaling and \n",
      "# accuracy characteristics vs. other algos? \n",
      "neighbors = 1\n",
      "numExamples = 2000 # we'll be waiting quite a while if we use 60K\n",
      "knn = KNeighborsClassifier(neighbors)\n",
      "mini_train_data, mini_train_labels = X[:numExamples], Y[:numExamples] \n",
      "start_time = time.time()\n",
      "knn.fit(mini_train_data, mini_train_labels)\n",
      "print 'Train time = %.2f' %(time.time() - start_time)\n",
      "start_time = time.time()\n",
      "accuracy = knn.score(test_data, test_labels)\n",
      "print 'Accuracy = %.4f' %(accuracy)\n",
      "print 'Prediction time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train time = 0.10\n",
        "prediction time = 31.84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy = 0.9089\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We'll start in Theano with implententing logistic regression.  \n",
      "# Recall the four key components: (1) parms, (2) model, (3) cost, and (4) objective. \n",
      "\n",
      "## (1) Parms \n",
      "# Init weights to small, but non-zero, values.\n",
      "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))\n",
      "\n",
      "\n",
      "## (2) Model\n",
      "# Theano objects accessed with standard Python variables\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "# Two things to note here.\n",
      "# First, logistic regression can be thought of as a neural net with no hidden layers.  So the output values are \n",
      "# just the dot product of the inputs and the edge weights.\n",
      "# Second, we have 10 classes.  So we can either train separate 1 vs all classification using sigmoid activation, \n",
      "# which would be a hassle, or we can use the softmax activation, which is essentially a multi-class version of sigmoid. \n",
      "\n",
      "\n",
      "def model(X, w):\n",
      "    return T.nnet.softmax(T.dot(X, w))\n",
      "y_hat = model(X, w)\n",
      "\n",
      "\n",
      "## (3) Cost\n",
      "# Cross entropy only considers the error between the true class and the prediction, and not the errors for the false \n",
      "# classes.  This tends to cause the network to converge faster.\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "\n",
      "## (4) Objective\n",
      "# Minimization using gradient descent.\n",
      "alpha = 0.01\n",
      "gradient = T.grad(cost=cost, wrt=w)\n",
      "update = [[w, w - gradient * alpha]] \n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) # computes cost, then runs update\n",
      "y_pred = T.argmax(y_hat, axis=1) # select largest probability as prediction\n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "\n",
      "def gradientDescent(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    for i in range(epochs):\n",
      "        start_time = time.time()\n",
      "        cost = train(train_data[0:len(train_data)], train_labels_b[0:len(train_data)])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))\n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "\n",
      "gradientDescent(50)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1) accuracy = 0.1288\n",
        "2) accuracy = 0.1554"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3) accuracy = 0.1841"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4) accuracy = 0.2176"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5) accuracy = 0.2456"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6) accuracy = 0.2763"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7) accuracy = 0.3122"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8) accuracy = 0.3490"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9) accuracy = 0.3856"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10) accuracy = 0.4207"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "11) accuracy = 0.4547"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12) accuracy = 0.4828"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "13) accuracy = 0.5072"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "14) accuracy = 0.5294"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "15) accuracy = 0.5490"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "16) accuracy = 0.5667"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17) accuracy = 0.5798"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "18) accuracy = 0.5918"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "19) accuracy = 0.6034"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "20) accuracy = 0.6146"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "21) accuracy = 0.6242"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "22) accuracy = 0.6327"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "23) accuracy = 0.6417"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "24) accuracy = 0.6499"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25) accuracy = 0.6564"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "26) accuracy = 0.6622"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27) accuracy = 0.6684"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "28) accuracy = 0.6738"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "29) accuracy = 0.6778"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30) accuracy = 0.6822"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "31) accuracy = 0.6864"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "32) accuracy = 0.6917"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "33) accuracy = 0.6964"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34) accuracy = 0.6993"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35) accuracy = 0.7026"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "36) accuracy = 0.7061"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "37) accuracy = 0.7089"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38) accuracy = 0.7100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "39) accuracy = 0.7125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40) accuracy = 0.7151"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "41) accuracy = 0.7178"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42) accuracy = 0.7194"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "43) accuracy = 0.7209"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "44) accuracy = 0.7223"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "45) accuracy = 0.7243"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "46) accuracy = 0.7266"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47) accuracy = 0.7278"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48) accuracy = 0.7297"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "49) accuracy = 0.7319"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50) accuracy = 0.7332"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train time = 54.29\n",
        "predict time = 0.07"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Let's switch to SGD and observe the impact. \n",
      "\n",
      "## (1) Parms\n",
      "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))\n",
      "\n",
      "## (2) Model\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "def model(X, w):\n",
      "    return T.nnet.softmax(T.dot(X, w))\n",
      "y_hat = model(X, w)\n",
      "\n",
      "## (3) Cost\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "## (4) Objective\n",
      "alpha = 0.01\n",
      "gradient = T.grad(cost=cost, wrt=w)\n",
      "update = [[w, w - gradient * alpha]] \n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) \n",
      "y_pred = T.argmax(y_hat, axis=1) \n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "## Play with this value and notice the impact.\n",
      "miniBatchSize = 1 \n",
      "def gradientDescentStochastic(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    start_time = time.time()\n",
      "    for i in range(epochs):       \n",
      "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
      "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))     \n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "    \n",
      "gradientDescentStochastic(10)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1) accuracy = 0.9049\n",
        "2) accuracy = 0.9093"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3) accuracy = 0.9108"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4) accuracy = 0.9113"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5) accuracy = 0.9121"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6) accuracy = 0.9134"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7) accuracy = 0.9139"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8) accuracy = 0.9144"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9) accuracy = 0.9149"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10) accuracy = 0.9150"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train time = 416.83\n",
        "predict time = 0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Now let's add a hidden layer (two layer neural net).\n",
      "\n",
      "## (1) Parms\n",
      "# Try playing with this value.\n",
      "numHiddenNodes = 400 \n",
      "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
      "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
      "params = [w_1, w_2]\n",
      "\n",
      "\n",
      "## (2) Model\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "# Two notes:\n",
      "# First, feed forward is the composition of layers (dot product + activation function)\n",
      "# Second, activation on the hidden layer still uses sigmoid\n",
      "def model(X, w_1, w_2):\n",
      "    return T.nnet.softmax(T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2))\n",
      "y_hat = model(X, w_1, w_2)\n",
      "\n",
      "\n",
      "## (3) Cost...same as logistic regression\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "\n",
      "## (4) Minimization.  Update rule changes to backpropagation.\n",
      "alpha = 0.01\n",
      "def backprop(cost, w):\n",
      "    grads = T.grad(cost=cost, wrt=w)\n",
      "    updates = []\n",
      "    for w1, grad in zip(w, grads):\n",
      "        updates.append([w1, w1 - grad * alpha])\n",
      "    return updates\n",
      "update = backprop(cost, params)\n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
      "y_pred = T.argmax(y_hat, axis=1)\n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "miniBatchSize = 1 \n",
      "def gradientDescentStochastic(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    start_time = time.time()\n",
      "    for i in range(epochs):\n",
      "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
      "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))\n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "\n",
      "gradientDescentStochastic(10)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1) accuracy = 0.9146\n",
        "2) accuracy = 0.9383"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3) accuracy = 0.9514"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4) accuracy = 0.9584"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5) accuracy = 0.9645"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6) accuracy = 0.9684"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7) accuracy = 0.9703"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8) accuracy = 0.9724"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9) accuracy = 0.9742"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10) accuracy = 0.9752"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train time = 9832.29\n",
        "predict time = 0.24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "PART 2: Ideas from 2010 onward"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As interest in bigger and deeper networks has increased, a couple of tricks have emerged and become standard practice.  Let's look at two of those--rectifier activation and dropout noise--that we'll use with deep networks.\n",
      "\n",
      "For a more in-depth examination of the topic, check out this 1-day tutorial from KDD2014:\n",
      "\n",
      "Part 1: http://videolectures.net/kdd2014_bengio_deep_learning/\n",
      "\n",
      "Part 2: http://videolectures.net/tcmm2014_taylor_deep_learning/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## A curiousity: what happens if we simply add a third layer?\n",
      "\n",
      "## (1) Parms\n",
      "numHiddenNodes = 400 \n",
      "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
      "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numHiddenNodes))*.01)))\n",
      "w_3 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
      "params = [w_1, w_2, w_3]\n",
      "\n",
      "## (2) Model\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "def model(X, w_1, w_2, w_3):\n",
      "    return T.nnet.softmax(T.dot(T.nnet.sigmoid(T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2)), w_3))\n",
      "y_hat = model(X, w_1, w_2, w_3)\n",
      "\n",
      "\n",
      "## (3) Cost...same as logistic regression\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "\n",
      "## (4) Minimization.  Update rule changes to backpropagation.\n",
      "alpha = 0.01\n",
      "def backprop(cost, w):\n",
      "    grads = T.grad(cost=cost, wrt=w)\n",
      "    updates = []\n",
      "    for w1, grad in zip(w, grads):\n",
      "        updates.append([w1, w1 - grad * alpha])\n",
      "    return updates\n",
      "update = backprop(cost, params)\n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
      "y_pred = T.argmax(y_hat, axis=1)\n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "miniBatchSize = 1 \n",
      "def gradientDescentStochastic(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    start_time = time.time()\n",
      "    for i in range(epochs):\n",
      "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
      "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))\n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "\n",
      "gradientDescentStochastic(10)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1) accuracy = 0.6451\n",
        "2) accuracy = 0.8932"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3) accuracy = 0.9289"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4) accuracy = 0.9455"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5) accuracy = 0.9530"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6) accuracy = 0.9587"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7) accuracy = 0.9631"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8) accuracy = 0.9650"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9) accuracy = 0.9677"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10) accuracy = 0.9696"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train time = 16779.59\n",
        "predict time = 0.46"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Activation Revisted"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we revisit adding layers, let's look at a recent idea around activation closely associated with deep learning.  In 2010, in a paper published at NIPS (https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf), Yoshua Bengio showed that rectifier activation works better empirically than sigmoid activation when used in the hidden layers.  \n",
      "\n",
      "The rectifier activation is simple: f(x)=max(0,x).  Intuitively, the difference is that as a sigmoid activated node approaches 1 it stops learning even if error continues to be propagated to it, whereas the rectifier activated node continue to learn (at least in the positive direction).  It is not completely understood (per Yoshua Bengio) why this helps, but there are some theories being explored including as related to the benefits of sparse representations in networks. (http://www.iro.umontreal.ca/~bengioy/talks/KDD2014-tutorial.pdf).  Rectifiers also speed up training.\n",
      "\n",
      "Although the paper was published in 2010, the technique didn't gain widespread adoption until 2012 when members of Hinton's group spread the word, including with this Kaggle entry: http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## 2-layer NN with rectify activation on the hidden layer.\n",
      "\n",
      "## (1) Parms\n",
      "numHiddenNodes = 400 \n",
      "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
      "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
      "params = [w_1, w_2]\n",
      "\n",
      "\n",
      "## (2) Model\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "\n",
      "def model(X, w_1, w_2):\n",
      "    return T.nnet.softmax(T.dot(T.maximum(T.dot(X, w_1), 0.), w_2))\n",
      "y_hat = model(X, w_1, w_2)\n",
      "\n",
      "\n",
      "## (3) Cost...same as logistic regression\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "\n",
      "## (4) Minimization.  Update rule changes to backpropagation.\n",
      "alpha = 0.01\n",
      "def backprop(cost, w):\n",
      "    grads = T.grad(cost=cost, wrt=w)\n",
      "    updates = []\n",
      "    for w1, grad in zip(w, grads):\n",
      "        updates.append([w1, w1 - grad * alpha])\n",
      "    return updates\n",
      "update = backprop(cost, params)\n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
      "y_pred = T.argmax(y_hat, axis=1)\n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "miniBatchSize = 1 \n",
      "def gradientDescentStochastic(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    start_time = time.time()\n",
      "    for i in range(epochs):\n",
      "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
      "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))\n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "\n",
      "gradientDescentStochastic(10)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1) accuracy = 0.9650\n",
        "2) accuracy = 0.9737"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3) accuracy = 0.9770"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4) accuracy = 0.9792"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5) accuracy = 0.9796"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6) accuracy = 0.9782"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7) accuracy = 0.9809"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8) accuracy = 0.9795"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9) accuracy = 0.9797"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10) accuracy = 0.9808"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train time = 25374.68\n",
        "predict time = 0.22"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Maxout"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As an exercise, switch to Maxout (or Max Pooling) activiation.  Maxout activation just selects the max input as the output.  Maxout is a type of pooling, a technique which performs particularly well for vision problems. (http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf, http://www.quora.com/What-is-impact-of-different-pooling-methods-in-convolutional-neural-networks).  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Noise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A second trick closely associated with deep learning, and that is now commonplace, is called 'Dropouts'.  The idea is that instead of (or in addition to) adding noise to our inputs, we add noise by having each node return 0 with a certain probability during training.  This trick both improves generalization in large networks and speeds up training.\n",
      "\n",
      "Hinton introduced the idea in 2012 and gave an explanation of why it's similar to bagging (http://arxiv.org/pdf/1207.0580v1.pdf)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dropouts\n",
      "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
      "\n",
      "## (1) Parms\n",
      "numHiddenNodes = 400 \n",
      "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
      "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
      "params = [w_1, w_2]\n",
      "\n",
      "\n",
      "## (2) Model\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "srng = RandomStreams()\n",
      "def dropout(X, p=0.):\n",
      "    if p > 0:\n",
      "        X *= srng.binomial(X.shape, p=1 - p)\n",
      "        X /= 1 - p\n",
      "    return X\n",
      "\n",
      "def model(X, w_1, w_2):\n",
      "    return T.nnet.softmax(T.dot(dropout(T.maximum(T.dot(dropout(X, 0.2), w_1),0.), 0.5), w_2))\n",
      "y_hat = model(X, w_1, w_2)\n",
      "\n",
      "\n",
      "## (3) Cost...same as logistic regression\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "\n",
      "## (4) Minimization.  Update rule changes to backpropagation.\n",
      "alpha = 0.01\n",
      "def backprop(cost, w):\n",
      "    grads = T.grad(cost=cost, wrt=w)\n",
      "    updates = []\n",
      "    for w1, grad in zip(w, grads):\n",
      "        updates.append([w1, w1 - grad * alpha])\n",
      "    return updates\n",
      "update = backprop(cost, params)\n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
      "y_pred = T.argmax(y_hat, axis=1)\n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "miniBatchSize = 1\n",
      "def gradientDescentStochastic(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    start_time = time.time()\n",
      "    for i in range(epochs):\n",
      "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
      "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))\n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "\n",
      "gradientDescentStochastic(10)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1) accuracy = 0.9229\n",
        "2) accuracy = 0.9375"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3) accuracy = 0.9449"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4) accuracy = 0.9489"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5) accuracy = 0.9488"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6) accuracy = 0.9532"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7) accuracy = 0.9478"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8) accuracy = 0.9545"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9) accuracy = 0.9523"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10) accuracy = 0.9545"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train time = 3694.47\n",
        "predict time = 0.65"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/theano/sandbox/rng_mrg.py:768: UserWarning: MRG_RandomStreams Can't determine #streams from size (Shape.0), guessing 60*256\n",
        "  nstreams = self.n_streams(size)\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's add back in that third layer\n",
      "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
      "\n",
      "## (1) Parms\n",
      "numHiddenNodes = 600 \n",
      "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
      "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numHiddenNodes))*.01)))\n",
      "w_3 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
      "params = [w_1, w_2, w_3]\n",
      "\n",
      "\n",
      "## (2) Model\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "srng = RandomStreams()\n",
      "def dropout(X, p=0.):\n",
      "    if p > 0:\n",
      "        X *= srng.binomial(X.shape, p=1 - p)\n",
      "        X /= 1 - p\n",
      "    return X\n",
      "def model(X, w_1, w_2, w_3):\n",
      "    return T.nnet.softmax(T.dot(dropout(T.maximum(T.dot(dropout(T.maximum(T.dot(dropout(X, 0.2), w_1),0.), 0.5), w_2),0.), 0.5), w_3))\n",
      "\n",
      "y_hat = model(X, w_1, w_2, w_3)\n",
      "\n",
      "\n",
      "## (3) Cost...same as logistic regression\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "\n",
      "## (4) Minimization.  Update rule changes to backpropagation.\n",
      "alpha = 0.01\n",
      "def backprop(cost, w):\n",
      "    grads = T.grad(cost=cost, wrt=w)\n",
      "    updates = []\n",
      "    for w1, grad in zip(w, grads):\n",
      "        updates.append([w1, w1 - grad * alpha])\n",
      "    return updates\n",
      "update = backprop(cost, params)\n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
      "y_pred = T.argmax(y_hat, axis=1)\n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "miniBatchSize = 1\n",
      "def gradientDescentStochastic(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    start_time = time.time()\n",
      "    for i in range(epochs):\n",
      "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
      "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))\n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "\n",
      "gradientDescentStochastic(10)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}