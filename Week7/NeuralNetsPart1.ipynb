{
 "metadata": {
  "name": "",
  "signature": "sha256:d3387f2e2cd5b08092fab3a4a54a08cebd29ffe2132b81730a4dd25a12470b84"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Digit Classification with Neural Networks (Part 1)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interest in neural nets, and in particular those with more than than one hidden layer, has been surging in recent years.  In this notebook we will be revisiting the problem of digit classification on the MNIST data.  We will be introducing a new Python library, Theano, for working with neural nets.  Theano is a popular choice as the same code can be run on either CPUs or GPUs.  GPUs greatly speed up the training and prediction, and easily available (Amazon even offers GPU machines on EC2).\n",
      "\n",
      "In part 1, we will review the basics of neural nets, and introduce Theano.  In part 2, we will investigate more advanced topics in neural nets, including  deep learning.  I'd encourage you to read this paper as well as a supplementary explanation of Theano (http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "# Familiar libraries.\n",
      "import numpy as np\n",
      "from sklearn.datasets import fetch_mldata\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import time\n",
      "\n",
      "# Take a moment to install Theano.  We will use it for building neural networks.\n",
      "import theano \n",
      "from theano import tensor as T\n",
      "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
      "print theano.config.device # We're using CPUs (for now)\n",
      "print theano.config.floatX # Should be 64 bit for CPUs\n",
      "\n",
      "np.random.seed(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cpu\n",
        "float64\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Repeating steps from Project 1 to prepare mnist dataset. \n",
      "mnist = fetch_mldata('MNIST original', data_home='~/datasets/mnist')\n",
      "X, Y = mnist.data, mnist.target\n",
      "X = X / 255.0\n",
      "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
      "X, Y = X[shuffle], Y[shuffle]\n",
      "test_data, test_labels = X[60000:], Y[60000:]\n",
      "train_data, train_labels = X[:60000], Y[:60000]\n",
      "numFeatures = train_data[1].size\n",
      "numTrainExamples = train_data.shape[0]\n",
      "numTestExamples = test_data.shape[0]\n",
      "print 'Features = %d' %(numFeatures)\n",
      "print 'Train set = %d' %(numTrainExamples)\n",
      "print 'Test set = %d' %(numTestExamples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Features = 784\n",
        "Train set = 60000\n",
        "Test set = 10000\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert labels into a set of binary variables, one for each class (sometimes called a 1-of-n encoding).  \n",
      "# This makes working with NNs easier: there will be one output node for each class.\n",
      "def binarizeY(data):\n",
      "    binarized_data = np.zeros((data.size,10))\n",
      "    for j in range(0,data.size):\n",
      "        feature = data[j:j+1]\n",
      "        i = feature.astype(np.int64) \n",
      "        binarized_data[j,i]=1\n",
      "    return binarized_data\n",
      "train_labels_b = binarizeY(train_labels)\n",
      "test_labels_b = binarizeY(test_labels)\n",
      "numClasses = train_labels_b[1].size\n",
      "print 'Classes = %d' %(numClasses)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Classes = 10\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Lets start with a simple KNN model to establish a baseline accuracy.\n",
      "# Question: You've seen a number of different machine learning algos.  What's your intuition about KNN scaling and \n",
      "# accuracy characteristics vs. other algos? \n",
      "neighbors = 1\n",
      "numExamples = 2000 # we'll be waiting quite a while if we use 60K\n",
      "knn = KNeighborsClassifier(neighbors)\n",
      "mini_train_data, mini_train_labels = X[:numExamples], Y[:numExamples] \n",
      "start_time = time.time()\n",
      "knn.fit(mini_train_data, mini_train_labels)\n",
      "print 'Train time = %.2f' %(time.time() - start_time)\n",
      "start_time = time.time()\n",
      "accuracy = knn.score(test_data, test_labels)\n",
      "print 'Accuracy = %.4f' %(accuracy)\n",
      "print 'Prediction time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "train time = 0.10\n",
        "prediction time = 31.84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accuracy = 0.9089\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We'll start in Theano with implententing logistic regression.  \n",
      "# Recall the four key components: (1) parms, (2) model, (3) cost, and (4) objective. \n",
      "\n",
      "## (1) Parms \n",
      "# Init weights to small, but non-zero, values.\n",
      "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.001)))\n",
      "\n",
      "\n",
      "## (2) Model\n",
      "# Theano objects accessed with standard Python variables\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "# Two things to note here.\n",
      "# First, logistic regression can be thought of as a neural net with no hidden layers.  So the output values are \n",
      "# just the dot product of the inputs and the edge weights.\n",
      "# Second, we have 10 classes.  So we can either train separate 1 vs all classification using sigmoid activation, \n",
      "# which would be a hassle, or we can use the softmax activation, which is essentially a multi-class version of sigmoid. \n",
      "def model(X, w):\n",
      "    return T.nnet.softmax(T.dot(X, w))\n",
      "y_hat = model(X, w)\n",
      "\n",
      "\n",
      "## (3) Cost\n",
      "# Cross entropy only considers the error between the true class and the prediction, and not the errors for the false \n",
      "# classes.  This tends to cause the network to converge faster.\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "\n",
      "## (4) Objective\n",
      "# Minimization using gradient descent.\n",
      "alpha = 0.05\n",
      "gradient = T.grad(cost=cost, wrt=w)\n",
      "update = [[w, w - gradient * alpha]] \n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) # computes cost, then runs update\n",
      "y_pred = T.argmax(y_hat, axis=1) # select largest probability as prediction\n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "\n",
      "def gradientDescent(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    for i in range(epochs):\n",
      "        start_time = time.time()\n",
      "        cost = train(train_data[0:len(train_data)], train_labels_b[0:len(train_data)])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))\n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "\n",
      "gradientDescent(5)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1) accuracy = 0.6557\n",
        "2) accuracy = 0.6809"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3) accuracy = 0.6947"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4) accuracy = 0.7098"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5) accuracy = 0.7199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6) accuracy = 0.7280"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7) accuracy = 0.7342"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8) accuracy = 0.7399"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9) accuracy = 0.7426"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10) accuracy = 0.7462"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train time = 10.12\n",
        "predict time = 0.06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Let's switch to SGD and observe the impact. \n",
      "\n",
      "## (1) Parms\n",
      "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.001)))\n",
      "\n",
      "## (2) Model\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "def model(X, w):\n",
      "    return T.nnet.softmax(T.dot(X, w))\n",
      "y_hat = model(X, w)\n",
      "\n",
      "## (3) Cost\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "## (4) Objective\n",
      "alpha = 0.01\n",
      "gradient = T.grad(cost=cost, wrt=w)\n",
      "update = [[w, w - gradient * alpha]] \n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) \n",
      "y_pred = T.argmax(y_hat, axis=1) \n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "## Play with this value and notice the impact.\n",
      "miniBatchSize = 10 \n",
      "def gradientDescentStochastic(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    start_time = time.time()\n",
      "    for i in range(epochs):       \n",
      "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
      "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))     \n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "    \n",
      "gradientDescentStochastic(5)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1) accuracy = 0.8909\n",
        "2) accuracy = 0.9007"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3) accuracy = 0.9057"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4) accuracy = 0.9087"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5) accuracy = 0.9111"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6) accuracy = 0.9127"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7) accuracy = 0.9146"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8) accuracy = 0.9158"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9) accuracy = 0.9158"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10) accuracy = 0.9165"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train time = 117.03\n",
        "predict time = 0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Now let's add a hidden layer (two layer neural net).\n",
      "\n",
      "## (1) Parms\n",
      "# Try playing with this value.\n",
      "numHiddenNodes = 400 \n",
      "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.1)))\n",
      "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.1)))\n",
      "params = [w_1, w_2]\n",
      "\n",
      "\n",
      "## (2) Model\n",
      "X = T.fmatrix()\n",
      "Y = T.fmatrix()\n",
      "# Two notes:\n",
      "# First, feed forward is the composition of layers (dot product + activation function)\n",
      "# Second, activation on the hidden layer still uses sigmoid\n",
      "def model(X, w_1, w_2):\n",
      "    return T.nnet.softmax(T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2))\n",
      "y_hat = model(X, w_1, w_2)\n",
      "\n",
      "\n",
      "## (3) Cost...same as logistic regression\n",
      "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
      "\n",
      "\n",
      "## (4) Minimization.  Update rule changes to backpropagation.\n",
      "alpha = 0.05\n",
      "def backprop(cost, w):\n",
      "    grads = T.grad(cost=cost, wrt=w)\n",
      "    updates = []\n",
      "    for w1, grad in zip(w, grads):\n",
      "        updates.append([w1, w1 - grad * alpha])\n",
      "    return updates\n",
      "update = backprop(cost, params)\n",
      "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
      "y_pred = T.argmax(y_hat, axis=1)\n",
      "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
      "\n",
      "miniBatchSize = 10 \n",
      "def gradientDescentStochastic(epochs):\n",
      "    trainTime = 0.0\n",
      "    predictTime = 0.0\n",
      "    start_time = time.time()\n",
      "    for i in range(epochs):\n",
      "        for start, end in zip(range(0, len(train_data), miniBatchSize), range(miniBatchSize, len(train_data), miniBatchSize)):\n",
      "            cost = train(train_data[start:end], train_labels_b[start:end])\n",
      "        trainTime =  trainTime + (time.time() - start_time)\n",
      "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data)))\n",
      "    print 'train time = %.2f' %(trainTime)\n",
      "\n",
      "gradientDescentStochastic(5)\n",
      "\n",
      "start_time = time.time()\n",
      "predict(test_data)   \n",
      "print 'predict time = %.2f' %(time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1) accuracy = 0.9128\n",
        "2) accuracy = 0.9281"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3) accuracy = 0.9365"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4) accuracy = 0.9436"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5) accuracy = 0.9497"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6) accuracy = 0.9541"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7) accuracy = 0.9580"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8) accuracy = 0.9618"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9) accuracy = 0.9639"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10) accuracy = 0.9661"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "train time = 1035.79\n",
        "predict time = 0.28"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Question: What would happen if we only used 2000 examples as we did with KNN?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}